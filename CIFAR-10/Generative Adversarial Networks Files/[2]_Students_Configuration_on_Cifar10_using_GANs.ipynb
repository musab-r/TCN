{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[2] Students Configuration on Cifar10 using GANs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAi6k17QhC0g"
      },
      "source": [
        "# **Distilling Knowledge In Multiple Students Using GANs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imgZBevn_klO"
      },
      "source": [
        "# %tensorflow_version 1.x\n",
        "# !pip install --upgrade opencv-python==3.4.2.17\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "import tensorflow.keras.backend as K\n",
        "# import os\n",
        "from tensorflow.keras.datasets import fashion_mnist,mnist,cifar10\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras.layers import Conv2D,Activation,BatchNormalization,UpSampling2D,Embedding,ZeroPadding2D, Input, Flatten, Dense, Reshape, LeakyReLU, Dropout,MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras.utils import Progbar\n",
        "from keras.initializers import RandomNormal\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from keras.utils import np_utils\n",
        "from tensorflow.keras import utils as np_utils\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0-DYwB8kiFk"
      },
      "source": [
        "nb_classes = 10\n",
        "batch_size = 128\n",
        "maxepoches = 250\n",
        "learning_rate = 0.1\n",
        "lr_decay = 1e-6\n",
        "lr_drop = 20\n",
        "def lr_scheduler(epoch):\n",
        "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e04yi6rW_qJg"
      },
      "source": [
        "#Loading and splitting the dataset into train, validation and test\n",
        "(X_Train, y_Train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_Train, y_Train, test_size=0.20)\n",
        "# convert y_train and y_test to categorical binary values \n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
        "y_test = np_utils.to_categorical(y_test, nb_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj_XM_dfmqnV",
        "outputId": "591466fa-f46e-46c5-c0dc-1be90026537d"
      },
      "source": [
        "X_Train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "443_UL2p_qyQ"
      },
      "source": [
        "# Reshape them to batch_size, width,height,#channels\n",
        "X_train = X_train.reshape(40000, 32, 32, 3)\n",
        "X_val = X_val.reshape(10000, 32, 32, 3)\n",
        "X_test = X_test.reshape(10000, 32, 32, 3)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Normalize the values\n",
        "X_train /= 255\n",
        "X_val /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aQn2hUFNVDY"
      },
      "source": [
        "# Teacher Network -- VGG16\n",
        "init=RandomNormal(mean=0,stddev=0.02)\n",
        "input_shape = (32, 32, 3) # Input shape of each image\n",
        "weight_decay = 0.0005\n",
        "\n",
        "def build_model():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same',\n",
        "                     input_shape=input_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256,kernel_regularizer=regularizers.l2(weight_decay), name='dense_1'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, name='dense_2'))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "\n",
        "teacher = build_model()\n",
        "sgd = SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "teacher.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp4il80HMRFn",
        "outputId": "c2960e97-675d-461c-8a56-6f4806871fd0"
      },
      "source": [
        "# teacher.fit(X_train,Y_train,batch_size=128,epochs=150,verbose=1,callbacks=[reduce_lr],validation_data=(X_val,Y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "313/313 [==============================] - 18s 36ms/step - loss: 24.6739 - accuracy: 0.1583 - val_loss: 19.0575 - val_accuracy: 0.1347\n",
            "Epoch 2/150\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 16.2624 - accuracy: 0.2884 - val_loss: 11.4850 - val_accuracy: 0.1458\n",
            "Epoch 3/150\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 9.4090 - accuracy: 0.3745 - val_loss: 6.9398 - val_accuracy: 0.2407\n",
            "Epoch 4/150\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 5.7181 - accuracy: 0.4646 - val_loss: 4.3902 - val_accuracy: 0.4041\n",
            "Epoch 5/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 3.7062 - accuracy: 0.5450 - val_loss: 3.0165 - val_accuracy: 0.5370\n",
            "Epoch 6/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 2.5910 - accuracy: 0.6161 - val_loss: 2.1274 - val_accuracy: 0.6332\n",
            "Epoch 7/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.9778 - accuracy: 0.6713 - val_loss: 2.1534 - val_accuracy: 0.5452\n",
            "Epoch 8/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.6913 - accuracy: 0.6902 - val_loss: 2.0979 - val_accuracy: 0.5380\n",
            "Epoch 9/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.5491 - accuracy: 0.7126 - val_loss: 1.7131 - val_accuracy: 0.6402\n",
            "Epoch 10/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4814 - accuracy: 0.7208 - val_loss: 1.6163 - val_accuracy: 0.6688\n",
            "Epoch 11/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4201 - accuracy: 0.7400 - val_loss: 1.6224 - val_accuracy: 0.6726\n",
            "Epoch 12/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.3928 - accuracy: 0.7567 - val_loss: 1.4548 - val_accuracy: 0.7285\n",
            "Epoch 13/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4015 - accuracy: 0.7558 - val_loss: 1.4736 - val_accuracy: 0.7273\n",
            "Epoch 14/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.3979 - accuracy: 0.7649 - val_loss: 1.4811 - val_accuracy: 0.7366\n",
            "Epoch 15/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 1.3982 - accuracy: 0.7717 - val_loss: 1.4460 - val_accuracy: 0.7635\n",
            "Epoch 16/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4210 - accuracy: 0.7755 - val_loss: 1.7883 - val_accuracy: 0.6753\n",
            "Epoch 17/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4113 - accuracy: 0.7762 - val_loss: 1.5202 - val_accuracy: 0.7514\n",
            "Epoch 18/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4366 - accuracy: 0.7809 - val_loss: 1.8119 - val_accuracy: 0.6664\n",
            "Epoch 19/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4481 - accuracy: 0.7840 - val_loss: 1.7083 - val_accuracy: 0.7042\n",
            "Epoch 20/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.4478 - accuracy: 0.7888 - val_loss: 1.5352 - val_accuracy: 0.7721\n",
            "Epoch 21/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.3407 - accuracy: 0.8221 - val_loss: 1.3726 - val_accuracy: 0.7853\n",
            "Epoch 22/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1814 - accuracy: 0.8478 - val_loss: 1.2374 - val_accuracy: 0.8114\n",
            "Epoch 23/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1480 - accuracy: 0.8428 - val_loss: 1.3052 - val_accuracy: 0.7886\n",
            "Epoch 24/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1294 - accuracy: 0.8436 - val_loss: 1.2485 - val_accuracy: 0.8113\n",
            "Epoch 25/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1310 - accuracy: 0.8463 - val_loss: 2.1039 - val_accuracy: 0.6527\n",
            "Epoch 26/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1341 - accuracy: 0.8518 - val_loss: 1.5302 - val_accuracy: 0.7373\n",
            "Epoch 27/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1564 - accuracy: 0.8488 - val_loss: 1.3000 - val_accuracy: 0.8010\n",
            "Epoch 28/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 1.1430 - accuracy: 0.8534 - val_loss: 1.3206 - val_accuracy: 0.7995\n",
            "Epoch 29/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1688 - accuracy: 0.8528 - val_loss: 1.3422 - val_accuracy: 0.7978\n",
            "Epoch 30/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1683 - accuracy: 0.8532 - val_loss: 1.3106 - val_accuracy: 0.8116\n",
            "Epoch 31/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1743 - accuracy: 0.8548 - val_loss: 1.2860 - val_accuracy: 0.8246\n",
            "Epoch 32/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1642 - accuracy: 0.8614 - val_loss: 1.3931 - val_accuracy: 0.7926\n",
            "Epoch 33/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1866 - accuracy: 0.8611 - val_loss: 1.7390 - val_accuracy: 0.7135\n",
            "Epoch 34/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1961 - accuracy: 0.8594 - val_loss: 1.3487 - val_accuracy: 0.8104\n",
            "Epoch 35/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1790 - accuracy: 0.8665 - val_loss: 1.3812 - val_accuracy: 0.8050\n",
            "Epoch 36/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1984 - accuracy: 0.8623 - val_loss: 1.3682 - val_accuracy: 0.8086\n",
            "Epoch 37/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1992 - accuracy: 0.8646 - val_loss: 1.3265 - val_accuracy: 0.8272\n",
            "Epoch 38/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.2070 - accuracy: 0.8640 - val_loss: 1.4179 - val_accuracy: 0.8021\n",
            "Epoch 39/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.2054 - accuracy: 0.8684 - val_loss: 1.3771 - val_accuracy: 0.8208\n",
            "Epoch 40/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.2204 - accuracy: 0.8649 - val_loss: 1.3823 - val_accuracy: 0.8145\n",
            "Epoch 41/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 1.1152 - accuracy: 0.8960 - val_loss: 1.2001 - val_accuracy: 0.8523\n",
            "Epoch 42/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9899 - accuracy: 0.9157 - val_loss: 1.1980 - val_accuracy: 0.8454\n",
            "Epoch 43/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9488 - accuracy: 0.9189 - val_loss: 1.2094 - val_accuracy: 0.8361\n",
            "Epoch 44/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9277 - accuracy: 0.9197 - val_loss: 1.1504 - val_accuracy: 0.8455\n",
            "Epoch 45/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9156 - accuracy: 0.9182 - val_loss: 1.1256 - val_accuracy: 0.8590\n",
            "Epoch 46/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.8960 - accuracy: 0.9222 - val_loss: 1.2205 - val_accuracy: 0.8335\n",
            "Epoch 47/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9187 - accuracy: 0.9130 - val_loss: 1.1893 - val_accuracy: 0.8405\n",
            "Epoch 48/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9124 - accuracy: 0.9144 - val_loss: 1.3704 - val_accuracy: 0.7911\n",
            "Epoch 49/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9135 - accuracy: 0.9155 - val_loss: 1.1436 - val_accuracy: 0.8524\n",
            "Epoch 50/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9141 - accuracy: 0.9175 - val_loss: 1.1920 - val_accuracy: 0.8397\n",
            "Epoch 51/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9282 - accuracy: 0.9171 - val_loss: 1.2515 - val_accuracy: 0.8307\n",
            "Epoch 52/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9229 - accuracy: 0.9173 - val_loss: 1.1352 - val_accuracy: 0.8566\n",
            "Epoch 53/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9330 - accuracy: 0.9165 - val_loss: 1.2263 - val_accuracy: 0.8373\n",
            "Epoch 54/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9361 - accuracy: 0.9170 - val_loss: 1.1581 - val_accuracy: 0.8578\n",
            "Epoch 55/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9463 - accuracy: 0.9162 - val_loss: 1.2716 - val_accuracy: 0.8269\n",
            "Epoch 56/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.9420 - accuracy: 0.9195 - val_loss: 1.3342 - val_accuracy: 0.8166\n",
            "Epoch 57/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9408 - accuracy: 0.9225 - val_loss: 1.2543 - val_accuracy: 0.8348\n",
            "Epoch 58/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9509 - accuracy: 0.9181 - val_loss: 1.1827 - val_accuracy: 0.8529\n",
            "Epoch 59/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.9423 - accuracy: 0.9222 - val_loss: 1.2515 - val_accuracy: 0.8425\n",
            "Epoch 60/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.9532 - accuracy: 0.9217 - val_loss: 1.2820 - val_accuracy: 0.8278\n",
            "Epoch 61/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.9042 - accuracy: 0.9369 - val_loss: 1.1657 - val_accuracy: 0.8620\n",
            "Epoch 62/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.8054 - accuracy: 0.9589 - val_loss: 1.1153 - val_accuracy: 0.8713\n",
            "Epoch 63/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.7725 - accuracy: 0.9618 - val_loss: 1.1017 - val_accuracy: 0.8733\n",
            "Epoch 64/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.7532 - accuracy: 0.9619 - val_loss: 1.0673 - val_accuracy: 0.8790\n",
            "Epoch 65/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.7268 - accuracy: 0.9637 - val_loss: 1.0816 - val_accuracy: 0.8682\n",
            "Epoch 66/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.7215 - accuracy: 0.9598 - val_loss: 1.0880 - val_accuracy: 0.8691\n",
            "Epoch 67/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.7112 - accuracy: 0.9573 - val_loss: 1.1823 - val_accuracy: 0.8433\n",
            "Epoch 68/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6999 - accuracy: 0.9601 - val_loss: 1.0928 - val_accuracy: 0.8626\n",
            "Epoch 69/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6980 - accuracy: 0.9594 - val_loss: 1.0736 - val_accuracy: 0.8610\n",
            "Epoch 70/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6885 - accuracy: 0.9583 - val_loss: 1.0186 - val_accuracy: 0.8748\n",
            "Epoch 71/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6896 - accuracy: 0.9577 - val_loss: 1.0676 - val_accuracy: 0.8623\n",
            "Epoch 72/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6794 - accuracy: 0.9603 - val_loss: 1.0984 - val_accuracy: 0.8587\n",
            "Epoch 73/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6662 - accuracy: 0.9628 - val_loss: 1.0876 - val_accuracy: 0.8532\n",
            "Epoch 74/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6853 - accuracy: 0.9546 - val_loss: 1.0312 - val_accuracy: 0.8695\n",
            "Epoch 75/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6782 - accuracy: 0.9569 - val_loss: 1.1039 - val_accuracy: 0.8533\n",
            "Epoch 76/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6772 - accuracy: 0.9575 - val_loss: 1.0998 - val_accuracy: 0.8560\n",
            "Epoch 77/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6741 - accuracy: 0.9581 - val_loss: 1.0860 - val_accuracy: 0.8638\n",
            "Epoch 78/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6831 - accuracy: 0.9539 - val_loss: 1.0599 - val_accuracy: 0.8592\n",
            "Epoch 79/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6762 - accuracy: 0.9568 - val_loss: 1.1236 - val_accuracy: 0.8483\n",
            "Epoch 80/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.6709 - accuracy: 0.9590 - val_loss: 1.0442 - val_accuracy: 0.8636\n",
            "Epoch 81/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.6462 - accuracy: 0.9684 - val_loss: 0.9832 - val_accuracy: 0.8837\n",
            "Epoch 82/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.5952 - accuracy: 0.9786 - val_loss: 0.9842 - val_accuracy: 0.8848\n",
            "Epoch 83/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.5710 - accuracy: 0.9826 - val_loss: 1.0104 - val_accuracy: 0.8780\n",
            "Epoch 84/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.5583 - accuracy: 0.9827 - val_loss: 0.9852 - val_accuracy: 0.8830\n",
            "Epoch 85/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.5428 - accuracy: 0.9843 - val_loss: 0.9679 - val_accuracy: 0.8905\n",
            "Epoch 86/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.5400 - accuracy: 0.9819 - val_loss: 0.9662 - val_accuracy: 0.8812\n",
            "Epoch 87/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.5295 - accuracy: 0.9821 - val_loss: 0.9730 - val_accuracy: 0.8844\n",
            "Epoch 88/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.5246 - accuracy: 0.9811 - val_loss: 0.9610 - val_accuracy: 0.8886\n",
            "Epoch 89/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.5047 - accuracy: 0.9842 - val_loss: 0.9475 - val_accuracy: 0.8837\n",
            "Epoch 90/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.5009 - accuracy: 0.9827 - val_loss: 0.9721 - val_accuracy: 0.8776\n",
            "Epoch 91/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4954 - accuracy: 0.9812 - val_loss: 1.0098 - val_accuracy: 0.8759\n",
            "Epoch 92/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4845 - accuracy: 0.9828 - val_loss: 0.9597 - val_accuracy: 0.8775\n",
            "Epoch 93/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4874 - accuracy: 0.9794 - val_loss: 0.9489 - val_accuracy: 0.8823\n",
            "Epoch 94/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4709 - accuracy: 0.9835 - val_loss: 0.9351 - val_accuracy: 0.8806\n",
            "Epoch 95/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4710 - accuracy: 0.9828 - val_loss: 0.9199 - val_accuracy: 0.8808\n",
            "Epoch 96/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4688 - accuracy: 0.9808 - val_loss: 0.9203 - val_accuracy: 0.8779\n",
            "Epoch 97/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4640 - accuracy: 0.9808 - val_loss: 0.9147 - val_accuracy: 0.8805\n",
            "Epoch 98/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4616 - accuracy: 0.9814 - val_loss: 0.9025 - val_accuracy: 0.8838\n",
            "Epoch 99/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4703 - accuracy: 0.9767 - val_loss: 0.8999 - val_accuracy: 0.8787\n",
            "Epoch 100/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4633 - accuracy: 0.9796 - val_loss: 0.9227 - val_accuracy: 0.8754\n",
            "Epoch 101/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4466 - accuracy: 0.9829 - val_loss: 0.8747 - val_accuracy: 0.8863\n",
            "Epoch 102/150\n",
            "313/313 [==============================] - 10s 34ms/step - loss: 0.4197 - accuracy: 0.9900 - val_loss: 0.8648 - val_accuracy: 0.8917\n",
            "Epoch 103/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4134 - accuracy: 0.9904 - val_loss: 0.8734 - val_accuracy: 0.8926\n",
            "Epoch 104/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.4000 - accuracy: 0.9923 - val_loss: 0.8843 - val_accuracy: 0.8935\n",
            "Epoch 105/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3956 - accuracy: 0.9925 - val_loss: 0.8961 - val_accuracy: 0.8942\n",
            "Epoch 106/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3912 - accuracy: 0.9920 - val_loss: 0.8765 - val_accuracy: 0.8937\n",
            "Epoch 107/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3862 - accuracy: 0.9921 - val_loss: 0.8723 - val_accuracy: 0.8959\n",
            "Epoch 108/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3763 - accuracy: 0.9937 - val_loss: 0.8772 - val_accuracy: 0.8935\n",
            "Epoch 109/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3733 - accuracy: 0.9929 - val_loss: 0.8763 - val_accuracy: 0.8943\n",
            "Epoch 110/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3672 - accuracy: 0.9942 - val_loss: 0.8922 - val_accuracy: 0.8893\n",
            "Epoch 111/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3680 - accuracy: 0.9920 - val_loss: 0.8739 - val_accuracy: 0.8940\n",
            "Epoch 112/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3615 - accuracy: 0.9927 - val_loss: 0.8714 - val_accuracy: 0.8926\n",
            "Epoch 113/150\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3588 - accuracy: 0.9919 - val_loss: 0.8460 - val_accuracy: 0.8962\n",
            "Epoch 114/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3558 - accuracy: 0.9916 - val_loss: 0.8635 - val_accuracy: 0.8898\n",
            "Epoch 115/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3520 - accuracy: 0.9919 - val_loss: 0.8681 - val_accuracy: 0.8901\n",
            "Epoch 116/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3431 - accuracy: 0.9927 - val_loss: 0.8576 - val_accuracy: 0.8901\n",
            "Epoch 117/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3407 - accuracy: 0.9924 - val_loss: 0.8544 - val_accuracy: 0.8920\n",
            "Epoch 118/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3335 - accuracy: 0.9931 - val_loss: 0.8755 - val_accuracy: 0.8902\n",
            "Epoch 119/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3368 - accuracy: 0.9911 - val_loss: 0.8475 - val_accuracy: 0.8924\n",
            "Epoch 120/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3303 - accuracy: 0.9925 - val_loss: 0.8348 - val_accuracy: 0.8911\n",
            "Epoch 121/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3269 - accuracy: 0.9920 - val_loss: 0.8072 - val_accuracy: 0.8984\n",
            "Epoch 122/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.3185 - accuracy: 0.9951 - val_loss: 0.8137 - val_accuracy: 0.8983\n",
            "Epoch 123/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.3155 - accuracy: 0.9950 - val_loss: 0.8085 - val_accuracy: 0.8994\n",
            "Epoch 124/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3094 - accuracy: 0.9961 - val_loss: 0.8076 - val_accuracy: 0.8987\n",
            "Epoch 125/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3063 - accuracy: 0.9963 - val_loss: 0.8152 - val_accuracy: 0.8993\n",
            "Epoch 126/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3031 - accuracy: 0.9965 - val_loss: 0.8156 - val_accuracy: 0.8985\n",
            "Epoch 127/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.3014 - accuracy: 0.9960 - val_loss: 0.8206 - val_accuracy: 0.8982\n",
            "Epoch 128/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2994 - accuracy: 0.9957 - val_loss: 0.8315 - val_accuracy: 0.8977\n",
            "Epoch 129/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2966 - accuracy: 0.9966 - val_loss: 0.8115 - val_accuracy: 0.8978\n",
            "Epoch 130/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.2906 - accuracy: 0.9971 - val_loss: 0.8026 - val_accuracy: 0.9020\n",
            "Epoch 131/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2893 - accuracy: 0.9972 - val_loss: 0.8092 - val_accuracy: 0.9008\n",
            "Epoch 132/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2900 - accuracy: 0.9962 - val_loss: 0.8165 - val_accuracy: 0.9007\n",
            "Epoch 133/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2852 - accuracy: 0.9969 - val_loss: 0.8220 - val_accuracy: 0.9003\n",
            "Epoch 134/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2852 - accuracy: 0.9965 - val_loss: 0.8430 - val_accuracy: 0.8976\n",
            "Epoch 135/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.2806 - accuracy: 0.9970 - val_loss: 0.8371 - val_accuracy: 0.8975\n",
            "Epoch 136/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2775 - accuracy: 0.9973 - val_loss: 0.8208 - val_accuracy: 0.9010\n",
            "Epoch 137/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2755 - accuracy: 0.9971 - val_loss: 0.8246 - val_accuracy: 0.8979\n",
            "Epoch 138/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2750 - accuracy: 0.9965 - val_loss: 0.8058 - val_accuracy: 0.8992\n",
            "Epoch 139/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2725 - accuracy: 0.9968 - val_loss: 0.8147 - val_accuracy: 0.8976\n",
            "Epoch 140/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.2727 - accuracy: 0.9963 - val_loss: 0.8189 - val_accuracy: 0.8978\n",
            "Epoch 141/150\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.2663 - accuracy: 0.9976 - val_loss: 0.8053 - val_accuracy: 0.9020\n",
            "Epoch 142/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2657 - accuracy: 0.9977 - val_loss: 0.8044 - val_accuracy: 0.9007\n",
            "Epoch 143/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2632 - accuracy: 0.9980 - val_loss: 0.8063 - val_accuracy: 0.9013\n",
            "Epoch 144/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2618 - accuracy: 0.9978 - val_loss: 0.7978 - val_accuracy: 0.9028\n",
            "Epoch 145/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2605 - accuracy: 0.9981 - val_loss: 0.8044 - val_accuracy: 0.9007\n",
            "Epoch 146/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2595 - accuracy: 0.9978 - val_loss: 0.8001 - val_accuracy: 0.9026\n",
            "Epoch 147/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2590 - accuracy: 0.9973 - val_loss: 0.8046 - val_accuracy: 0.9032\n",
            "Epoch 148/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2580 - accuracy: 0.9976 - val_loss: 0.8072 - val_accuracy: 0.9014\n",
            "Epoch 149/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2563 - accuracy: 0.9977 - val_loss: 0.8017 - val_accuracy: 0.9020\n",
            "Epoch 150/150\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.2554 - accuracy: 0.9973 - val_loss: 0.8150 - val_accuracy: 0.9009\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc01c477eb8>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UK9EtWGaGcr"
      },
      "source": [
        "teacher.load_weights(\"Cifar10_Teacher.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBZO-MXLmml_",
        "outputId": "35c8a00d-a373-4d84-b9be-72925e69f67d"
      },
      "source": [
        "loss, acc =teacher.evaluate(X_test, y_test, verbose=1)\n",
        "loss, acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.8343 - accuracy: 0.8992\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.834343671798706, 0.8992000222206116)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPVxVj6L_sCz"
      },
      "source": [
        "#Collect the dense vector from the previous layer output and store it in a different model\n",
        "teacher_WO_Softmax = Model(teacher.input, teacher.get_layer('dense_1').output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhcEQ0Z-_scF"
      },
      "source": [
        "#Extracting dense representation from the teacher network\n",
        "train_dense = teacher_WO_Softmax.predict(X_train)\n",
        "# val_dense = teacher_WO_Softmax.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG0WGCpM_suF"
      },
      "source": [
        "#Splitting the training dense vector among N students(in this case 2)\n",
        "\n",
        "# 2 Students Case\n",
        "# --------------------------------------------\n",
        "s1Train=train_dense[:,:128]\n",
        "s2Train=train_dense[:,128:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKx8VcoJxwH0",
        "outputId": "1644eb6d-f1f8-48c7-bda8-1fc8266576a5"
      },
      "source": [
        "def define_model(name):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3), name=name))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(16, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform',name='req'+name))\n",
        "\n",
        "    # compile model\n",
        "    # opt = SGD(lr=0.001, momentum=0.9)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "student1 = define_model('s1')\n",
        "student1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "s1 (Conv2D)                  (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                32784     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "reqs1 (Dense)                (None, 128)               2176      \n",
            "=================================================================\n",
            "Total params: 321,968\n",
            "Trainable params: 321,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bf51HQeYB19"
      },
      "source": [
        "# import np.random import random\n",
        "BATCH_SIZE=32\n",
        "def smooth_real_labels(y):\n",
        "    return y - 0.3+(np.random.random(y.shape)*0.5)\n",
        "def smooth_fake_labels(y):\n",
        "    return y + (0.3 * np.random.random(y.shape))\n",
        "def build_gan(gen,disc): \n",
        "    disc.trainable = False\n",
        "    input= Input(shape=input_shape)\n",
        "    output = gen(input)\n",
        "    output2= disc(output)\n",
        "    gan=Model(input,output2)\n",
        "\n",
        "    gan.compile('adam',loss=['binary_crossentropy','mse'],metrics=['accuracy'])\n",
        "\n",
        "    return gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DN9rlsCXBHl"
      },
      "source": [
        "def build_sdiscriminator():\n",
        "#   update the first line according to your dense chunk\n",
        "    input2 = Input(shape=(128,),name='input')\n",
        "    inp=Dense(128)(input2)\n",
        "\n",
        "    leaky_relu = LeakyReLU(alpha=0.2)(inp)\n",
        "    \n",
        "    conv3 = Dense(128,activation='relu')(leaky_relu)\n",
        "    b_n = BatchNormalization()(conv3)\n",
        "    # leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "    conv3 = Dense(128,activation='relu')(leaky_relu)\n",
        "    b_n = BatchNormalization()(conv3)\n",
        "    # leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "    conv3 = Dense(128,activation='relu')(b_n)\n",
        "    b_n = BatchNormalization()(conv3)\n",
        "    # leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "    conv4 = Dense(256,activation='relu')(b_n)\n",
        "    b_n = BatchNormalization()(conv4)\n",
        "    # leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "    conv4 = Dense(256,activation='relu')(b_n)\n",
        "    b_n = BatchNormalization()(conv4)\n",
        "    leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "    conv4 = Dense(512)(leaky_relu)\n",
        "    b_n = BatchNormalization()(conv4)\n",
        "    leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "    conv4 = Dense(512,activation='relu')(b_n)\n",
        "    b_n = BatchNormalization()(conv4)\n",
        "    leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "    conv4 = Dense(1024)(leaky_relu)\n",
        "    b_n = BatchNormalization()(conv4)\n",
        "    leaky_relu = LeakyReLU(alpha=0.2)(b_n)\n",
        "\n",
        "    dense = Dense(1,activation='sigmoid')(b_n)\n",
        "\n",
        "    output2=Dense(128)(b_n)\n",
        "    disc = Model(input2,[dense,output2])          \n",
        "    disc.compile(optd,loss=['binary_crossentropy','mse'],metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return disc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP0r1sO5ACHS"
      },
      "source": [
        "def training(generator,discriminator,gan,features,epo=20):\n",
        "    # Setup Models here\n",
        "    BATCH_SIZE = 128\n",
        "    discriminator.trainable = True\n",
        "    total_size = X_train.shape[0]\n",
        "    indices = np.arange(0,total_size ,BATCH_SIZE)\n",
        "    all_disc_loss = []\n",
        "    all_gen_loss = []\n",
        "    all_class_loss=[]\n",
        "    if total_size % BATCH_SIZE:\n",
        "        indices = indices[:-1]\n",
        "    for e in range(epo):\n",
        "        \n",
        "        progress_bar = Progbar(target=len(indices))\n",
        "        np.random.shuffle(indices)\n",
        "        epoch_gen_loss = []\n",
        "        epoch_disc_loss = []\n",
        "        epoch_class_loss= []\n",
        "        for i,index in enumerate(indices):\n",
        "        \n",
        "            # Write your code here\n",
        "            inputs=X_train[index:index+BATCH_SIZE]\n",
        "            real_image = features[index:index+BATCH_SIZE]\n",
        "            y_train = features[index:index+BATCH_SIZE]\n",
        "\n",
        "            y_real = np.ones((BATCH_SIZE,1))\n",
        "            y_fake = np.zeros((BATCH_SIZE,1))\n",
        "\n",
        "            #Generator Training\n",
        "            fake_images = generator.predict_on_batch(inputs)\n",
        "\n",
        "            #Disrciminator Training\n",
        "            disc_real_loss1,_,disc_real_loss2,_,_= discriminator.train_on_batch(real_image,[y_real,y_train])\n",
        "            disc_fake_loss1,_,disc_fake_loss2,_,_= discriminator.train_on_batch(fake_images,[y_fake,y_train])\n",
        "\n",
        "            #Gans Training\n",
        "            discriminator.trainable = False\n",
        "            gan_loss,_,gan_loss2,_,_ = gan.train_on_batch(inputs, [y_real,y_train])\n",
        "            gan_loss,_,gan_loss2,_,_ = gan.train_on_batch(inputs, [y_real,y_train])\n",
        "            gan_loss,_,gan_loss2,_,_ = gan.train_on_batch(inputs, [y_real,y_train])\n",
        "            gan_loss,_,gan_loss2,_,_ = gan.train_on_batch(inputs, [y_real,y_train])\n",
        "\n",
        "            discriminator.trainable = True\n",
        "\n",
        "            disc_loss = (disc_fake_loss1 + disc_real_loss1)/2\n",
        "            epoch_disc_loss.append(disc_loss)\n",
        "            progress_bar.update(i+1)\n",
        "\n",
        "            epoch_gen_loss.append((gan_loss))\n",
        "\n",
        "        avg_epoch_disc_loss = np.array(epoch_disc_loss).mean()\n",
        "        avg_epoch_gen_loss = np.array(epoch_gen_loss).mean()\n",
        "        all_disc_loss.append(avg_epoch_disc_loss)\n",
        "        all_gen_loss.append(avg_epoch_gen_loss)\n",
        "        print(\"Epoch: %d | Discriminator Loss: %f | Generator Loss: %f | \" % (e+1,avg_epoch_disc_loss,avg_epoch_gen_loss))\n",
        "\n",
        "    return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQbGnKgUiIxf"
      },
      "source": [
        "#  Reported results in the paper was achieved on training the network for 90+ epoch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQlldHWBPJMn"
      },
      "source": [
        "**2 Students**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7m6SoOWPNDH",
        "outputId": "66c8ec20-ab25-4556-d101-0c96724b84c1"
      },
      "source": [
        "optd = Adam(lr=0.0002)\n",
        "opt = Adam(lr=0.0002)\n",
        "\n",
        "discriminator1 = build_sdiscriminator()\n",
        "discriminator2 = build_sdiscriminator()\n",
        "s1=define_model(\"s1\")\n",
        "s2=define_model(\"s2\")\n",
        "\n",
        "gan1 = build_gan(s1,discriminator1)\n",
        "gan2 = build_gan(s2,discriminator2)\n",
        "s1 = training(s1,discriminator1,gan1,s1Train,epo=90)\n",
        "s2 = training(s2,discriminator2,gan2,s2Train,epo=98)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "312/312 [==============================] - 33s 101ms/step\n",
            "Epoch: 1 | Discriminator Loss: 1.609098 | Generator Loss: 2.042668 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 2 | Discriminator Loss: 1.311167 | Generator Loss: 1.923088 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 3 | Discriminator Loss: 1.172249 | Generator Loss: 1.722566 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 4 | Discriminator Loss: 1.083695 | Generator Loss: 1.602453 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 5 | Discriminator Loss: 1.038289 | Generator Loss: 1.526502 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 6 | Discriminator Loss: 0.999956 | Generator Loss: 1.442289 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 7 | Discriminator Loss: 0.975897 | Generator Loss: 1.396637 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 8 | Discriminator Loss: 0.955632 | Generator Loss: 1.354720 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 9 | Discriminator Loss: 0.939519 | Generator Loss: 1.326549 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 10 | Discriminator Loss: 0.924983 | Generator Loss: 1.285550 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 11 | Discriminator Loss: 0.912707 | Generator Loss: 1.252826 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 12 | Discriminator Loss: 0.902674 | Generator Loss: 1.228666 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 13 | Discriminator Loss: 0.893782 | Generator Loss: 1.215228 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 14 | Discriminator Loss: 0.885194 | Generator Loss: 1.198814 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 15 | Discriminator Loss: 0.877207 | Generator Loss: 1.182548 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 16 | Discriminator Loss: 0.870535 | Generator Loss: 1.173000 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 17 | Discriminator Loss: 0.866241 | Generator Loss: 1.160462 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 18 | Discriminator Loss: 0.863361 | Generator Loss: 1.153308 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 19 | Discriminator Loss: 0.858583 | Generator Loss: 1.141377 | \n",
            "312/312 [==============================] - 30s 98ms/step\n",
            "Epoch: 20 | Discriminator Loss: 0.853373 | Generator Loss: 1.141022 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 21 | Discriminator Loss: 0.850413 | Generator Loss: 1.125440 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 22 | Discriminator Loss: 0.845406 | Generator Loss: 1.119727 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 23 | Discriminator Loss: 0.843255 | Generator Loss: 1.117197 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 24 | Discriminator Loss: 0.839689 | Generator Loss: 1.109784 | \n",
            "312/312 [==============================] - 30s 96ms/step\n",
            "Epoch: 25 | Discriminator Loss: 0.837974 | Generator Loss: 1.110017 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 26 | Discriminator Loss: 0.835586 | Generator Loss: 1.103284 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 27 | Discriminator Loss: 0.833348 | Generator Loss: 1.095137 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 28 | Discriminator Loss: 0.830089 | Generator Loss: 1.096715 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 29 | Discriminator Loss: 0.827551 | Generator Loss: 1.092169 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 30 | Discriminator Loss: 0.824140 | Generator Loss: 1.083090 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 31 | Discriminator Loss: 0.823531 | Generator Loss: 1.081247 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 32 | Discriminator Loss: 0.819791 | Generator Loss: 1.081724 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 33 | Discriminator Loss: 0.817058 | Generator Loss: 1.074052 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 34 | Discriminator Loss: 0.817797 | Generator Loss: 1.076786 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 35 | Discriminator Loss: 0.816262 | Generator Loss: 1.076058 | \n",
            "312/312 [==============================] - 32s 103ms/step\n",
            "Epoch: 36 | Discriminator Loss: 0.814445 | Generator Loss: 1.070212 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 37 | Discriminator Loss: 0.813149 | Generator Loss: 1.068109 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 38 | Discriminator Loss: 0.812373 | Generator Loss: 1.062631 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 39 | Discriminator Loss: 0.810036 | Generator Loss: 1.057361 | \n",
            "312/312 [==============================] - 32s 104ms/step\n",
            "Epoch: 40 | Discriminator Loss: 0.808646 | Generator Loss: 1.053622 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 41 | Discriminator Loss: 0.806214 | Generator Loss: 1.050875 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 42 | Discriminator Loss: 0.803759 | Generator Loss: 1.048564 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 43 | Discriminator Loss: 0.801609 | Generator Loss: 1.046927 | \n",
            "312/312 [==============================] - 33s 105ms/step\n",
            "Epoch: 44 | Discriminator Loss: 0.800934 | Generator Loss: 1.042795 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 45 | Discriminator Loss: 0.801886 | Generator Loss: 1.047609 | \n",
            "312/312 [==============================] - 32s 104ms/step\n",
            "Epoch: 46 | Discriminator Loss: 0.800666 | Generator Loss: 1.044001 | \n",
            "312/312 [==============================] - 32s 104ms/step\n",
            "Epoch: 47 | Discriminator Loss: 0.798059 | Generator Loss: 1.040691 | \n",
            "312/312 [==============================] - 32s 103ms/step\n",
            "Epoch: 48 | Discriminator Loss: 0.797931 | Generator Loss: 1.040808 | \n",
            "312/312 [==============================] - 32s 103ms/step\n",
            "Epoch: 49 | Discriminator Loss: 0.797191 | Generator Loss: 1.037632 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 50 | Discriminator Loss: 0.796988 | Generator Loss: 1.040053 | \n",
            "312/312 [==============================] - 32s 103ms/step\n",
            "Epoch: 51 | Discriminator Loss: 0.795390 | Generator Loss: 1.033784 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 52 | Discriminator Loss: 0.795687 | Generator Loss: 1.039540 | \n",
            "312/312 [==============================] - 32s 103ms/step\n",
            "Epoch: 53 | Discriminator Loss: 0.795778 | Generator Loss: 1.040510 | \n",
            "312/312 [==============================] - 33s 105ms/step\n",
            "Epoch: 54 | Discriminator Loss: 0.796167 | Generator Loss: 1.039027 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 55 | Discriminator Loss: 0.796083 | Generator Loss: 1.040878 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 56 | Discriminator Loss: 0.792573 | Generator Loss: 1.037965 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 57 | Discriminator Loss: 0.791487 | Generator Loss: 1.034179 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 58 | Discriminator Loss: 0.789814 | Generator Loss: 1.026993 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 59 | Discriminator Loss: 0.788808 | Generator Loss: 1.030534 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 60 | Discriminator Loss: 0.790111 | Generator Loss: 1.030726 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 61 | Discriminator Loss: 0.789082 | Generator Loss: 1.027516 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 62 | Discriminator Loss: 0.788109 | Generator Loss: 1.024000 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 63 | Discriminator Loss: 0.786478 | Generator Loss: 1.020716 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 64 | Discriminator Loss: 0.788039 | Generator Loss: 1.023466 | \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 65 | Discriminator Loss: 0.784632 | Generator Loss: 1.019240 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 66 | Discriminator Loss: 0.783926 | Generator Loss: 1.012295 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 67 | Discriminator Loss: 0.783304 | Generator Loss: 1.011661 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 68 | Discriminator Loss: 0.783176 | Generator Loss: 1.009921 | \n",
            "312/312 [==============================] - 30s 96ms/step\n",
            "Epoch: 69 | Discriminator Loss: 0.780788 | Generator Loss: 1.008956 | \n",
            "312/312 [==============================] - 30s 98ms/step\n",
            "Epoch: 70 | Discriminator Loss: 0.780628 | Generator Loss: 1.010409 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 71 | Discriminator Loss: 0.781057 | Generator Loss: 1.010212 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 72 | Discriminator Loss: 0.780841 | Generator Loss: 1.008986 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 73 | Discriminator Loss: 0.781859 | Generator Loss: 1.015824 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 74 | Discriminator Loss: 0.780427 | Generator Loss: 1.015726 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 75 | Discriminator Loss: 0.780565 | Generator Loss: 1.007068 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 76 | Discriminator Loss: 0.780424 | Generator Loss: 1.009969 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 77 | Discriminator Loss: 0.779667 | Generator Loss: 1.009041 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 78 | Discriminator Loss: 0.779784 | Generator Loss: 1.007679 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 79 | Discriminator Loss: 0.779206 | Generator Loss: 1.009630 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 80 | Discriminator Loss: 0.777479 | Generator Loss: 1.005174 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 81 | Discriminator Loss: 0.776158 | Generator Loss: 0.999047 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 82 | Discriminator Loss: 0.777263 | Generator Loss: 1.003245 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 83 | Discriminator Loss: 0.775939 | Generator Loss: 1.007024 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 84 | Discriminator Loss: 0.778509 | Generator Loss: 1.010868 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 85 | Discriminator Loss: 0.777078 | Generator Loss: 1.004766 | \n",
            "312/312 [==============================] - 32s 103ms/step\n",
            "Epoch: 86 | Discriminator Loss: 0.775523 | Generator Loss: 0.998228 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 87 | Discriminator Loss: 0.775929 | Generator Loss: 1.006337 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 88 | Discriminator Loss: 0.775888 | Generator Loss: 1.000185 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 89 | Discriminator Loss: 0.774472 | Generator Loss: 1.000571 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 90 | Discriminator Loss: 0.774625 | Generator Loss: 0.998761 | \n",
            "312/312 [==============================] - 33s 99ms/step\n",
            "Epoch: 1 | Discriminator Loss: 1.414464 | Generator Loss: 1.810568 | \n",
            "312/312 [==============================] - 30s 98ms/step\n",
            "Epoch: 2 | Discriminator Loss: 1.286325 | Generator Loss: 1.837288 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 3 | Discriminator Loss: 1.186419 | Generator Loss: 1.706785 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 4 | Discriminator Loss: 1.054017 | Generator Loss: 1.509919 | \n",
            "312/312 [==============================] - 30s 98ms/step\n",
            "Epoch: 5 | Discriminator Loss: 1.011381 | Generator Loss: 1.439580 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 6 | Discriminator Loss: 0.979734 | Generator Loss: 1.398561 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 7 | Discriminator Loss: 0.961254 | Generator Loss: 1.366498 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 8 | Discriminator Loss: 0.942928 | Generator Loss: 1.338599 | \n",
            "312/312 [==============================] - 30s 96ms/step\n",
            "Epoch: 9 | Discriminator Loss: 0.928399 | Generator Loss: 1.303342 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 10 | Discriminator Loss: 0.916273 | Generator Loss: 1.284363 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 11 | Discriminator Loss: 0.903254 | Generator Loss: 1.252124 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 12 | Discriminator Loss: 0.892006 | Generator Loss: 1.230437 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 13 | Discriminator Loss: 0.885510 | Generator Loss: 1.211889 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 14 | Discriminator Loss: 0.877164 | Generator Loss: 1.191851 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 15 | Discriminator Loss: 0.872611 | Generator Loss: 1.174459 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 16 | Discriminator Loss: 0.865733 | Generator Loss: 1.163026 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 17 | Discriminator Loss: 0.861531 | Generator Loss: 1.155582 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 18 | Discriminator Loss: 0.856028 | Generator Loss: 1.148118 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 19 | Discriminator Loss: 0.849947 | Generator Loss: 1.130717 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 20 | Discriminator Loss: 0.845825 | Generator Loss: 1.124853 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 21 | Discriminator Loss: 0.842300 | Generator Loss: 1.118553 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 22 | Discriminator Loss: 0.837673 | Generator Loss: 1.103879 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 23 | Discriminator Loss: 0.835305 | Generator Loss: 1.099372 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 24 | Discriminator Loss: 0.830850 | Generator Loss: 1.093405 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 25 | Discriminator Loss: 0.828134 | Generator Loss: 1.084209 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 26 | Discriminator Loss: 0.825787 | Generator Loss: 1.085414 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 27 | Discriminator Loss: 0.821305 | Generator Loss: 1.079088 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 28 | Discriminator Loss: 0.817608 | Generator Loss: 1.071444 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 29 | Discriminator Loss: 0.817206 | Generator Loss: 1.063804 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 30 | Discriminator Loss: 0.812820 | Generator Loss: 1.052751 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 31 | Discriminator Loss: 0.810453 | Generator Loss: 1.045088 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 32 | Discriminator Loss: 0.809183 | Generator Loss: 1.040291 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 33 | Discriminator Loss: 0.806334 | Generator Loss: 1.039677 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 34 | Discriminator Loss: 0.804980 | Generator Loss: 1.033550 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 35 | Discriminator Loss: 0.802845 | Generator Loss: 1.026741 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 36 | Discriminator Loss: 0.801348 | Generator Loss: 1.020875 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 37 | Discriminator Loss: 0.799204 | Generator Loss: 1.022205 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 38 | Discriminator Loss: 0.796579 | Generator Loss: 1.015878 | \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 39 | Discriminator Loss: 0.795484 | Generator Loss: 1.019422 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 40 | Discriminator Loss: 0.793686 | Generator Loss: 1.012794 | \n",
            "312/312 [==============================] - 29s 95ms/step\n",
            "Epoch: 41 | Discriminator Loss: 0.792217 | Generator Loss: 1.006424 | \n",
            "312/312 [==============================] - 30s 96ms/step\n",
            "Epoch: 42 | Discriminator Loss: 0.790758 | Generator Loss: 1.005587 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 43 | Discriminator Loss: 0.790514 | Generator Loss: 1.002195 | \n",
            "312/312 [==============================] - 30s 95ms/step\n",
            "Epoch: 44 | Discriminator Loss: 0.787659 | Generator Loss: 0.998503 | \n",
            "312/312 [==============================] - 30s 98ms/step\n",
            "Epoch: 45 | Discriminator Loss: 0.785865 | Generator Loss: 0.995353 | \n",
            "312/312 [==============================] - 30s 95ms/step\n",
            "Epoch: 46 | Discriminator Loss: 0.785908 | Generator Loss: 0.995983 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 47 | Discriminator Loss: 0.783521 | Generator Loss: 0.995273 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 48 | Discriminator Loss: 0.781018 | Generator Loss: 0.985108 | \n",
            "312/312 [==============================] - 30s 96ms/step\n",
            "Epoch: 49 | Discriminator Loss: 0.780467 | Generator Loss: 0.980385 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 50 | Discriminator Loss: 0.779127 | Generator Loss: 0.984248 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 51 | Discriminator Loss: 0.778618 | Generator Loss: 0.979345 | \n",
            "312/312 [==============================] - 30s 96ms/step\n",
            "Epoch: 52 | Discriminator Loss: 0.777245 | Generator Loss: 0.980003 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 53 | Discriminator Loss: 0.777230 | Generator Loss: 0.978710 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 54 | Discriminator Loss: 0.775894 | Generator Loss: 0.972640 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 55 | Discriminator Loss: 0.776038 | Generator Loss: 0.975784 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 56 | Discriminator Loss: 0.774813 | Generator Loss: 0.968848 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 57 | Discriminator Loss: 0.773803 | Generator Loss: 0.972473 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 58 | Discriminator Loss: 0.772686 | Generator Loss: 0.971223 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 59 | Discriminator Loss: 0.770738 | Generator Loss: 0.966263 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 60 | Discriminator Loss: 0.771439 | Generator Loss: 0.969267 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 61 | Discriminator Loss: 0.770755 | Generator Loss: 0.966835 | \n",
            "312/312 [==============================] - 32s 101ms/step\n",
            "Epoch: 62 | Discriminator Loss: 0.770617 | Generator Loss: 0.968050 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 63 | Discriminator Loss: 0.770093 | Generator Loss: 0.966486 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 64 | Discriminator Loss: 0.768226 | Generator Loss: 0.961127 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 65 | Discriminator Loss: 0.767356 | Generator Loss: 0.960135 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 66 | Discriminator Loss: 0.767026 | Generator Loss: 0.957165 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 67 | Discriminator Loss: 0.767107 | Generator Loss: 0.956500 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 68 | Discriminator Loss: 0.767189 | Generator Loss: 0.963146 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 69 | Discriminator Loss: 0.766838 | Generator Loss: 0.958543 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 70 | Discriminator Loss: 0.765521 | Generator Loss: 0.960124 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 71 | Discriminator Loss: 0.764380 | Generator Loss: 0.959829 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 72 | Discriminator Loss: 0.765525 | Generator Loss: 0.961817 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 73 | Discriminator Loss: 0.763467 | Generator Loss: 0.958105 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 74 | Discriminator Loss: 0.763581 | Generator Loss: 0.958958 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 75 | Discriminator Loss: 0.762853 | Generator Loss: 0.956423 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 76 | Discriminator Loss: 0.761952 | Generator Loss: 0.955627 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 77 | Discriminator Loss: 0.763291 | Generator Loss: 0.956040 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 78 | Discriminator Loss: 0.761954 | Generator Loss: 0.952054 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 79 | Discriminator Loss: 0.760837 | Generator Loss: 0.952317 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 80 | Discriminator Loss: 0.759153 | Generator Loss: 0.951122 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 81 | Discriminator Loss: 0.759344 | Generator Loss: 0.949847 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 82 | Discriminator Loss: 0.759224 | Generator Loss: 0.942274 | \n",
            "312/312 [==============================] - 30s 97ms/step\n",
            "Epoch: 83 | Discriminator Loss: 0.759305 | Generator Loss: 0.948335 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 84 | Discriminator Loss: 0.757757 | Generator Loss: 0.945551 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 85 | Discriminator Loss: 0.757665 | Generator Loss: 0.944276 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 86 | Discriminator Loss: 0.756957 | Generator Loss: 0.944481 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 87 | Discriminator Loss: 0.758229 | Generator Loss: 0.946233 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 88 | Discriminator Loss: 0.757611 | Generator Loss: 0.946764 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 89 | Discriminator Loss: 0.758036 | Generator Loss: 0.946854 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 90 | Discriminator Loss: 0.756650 | Generator Loss: 0.946324 | \n",
            "312/312 [==============================] - 32s 102ms/step\n",
            "Epoch: 91 | Discriminator Loss: 0.755747 | Generator Loss: 0.939237 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 92 | Discriminator Loss: 0.756964 | Generator Loss: 0.940048 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 93 | Discriminator Loss: 0.757362 | Generator Loss: 0.948812 | \n",
            "312/312 [==============================] - 31s 101ms/step\n",
            "Epoch: 94 | Discriminator Loss: 0.757495 | Generator Loss: 0.952490 | \n",
            "312/312 [==============================] - 31s 98ms/step\n",
            "Epoch: 95 | Discriminator Loss: 0.756294 | Generator Loss: 0.945608 | \n",
            "312/312 [==============================] - 31s 100ms/step\n",
            "Epoch: 96 | Discriminator Loss: 0.755987 | Generator Loss: 0.940929 | \n",
            "312/312 [==============================] - 31s 99ms/step\n",
            "Epoch: 97 | Discriminator Loss: 0.754955 | Generator Loss: 0.943093 | \n",
            "312/312 [==============================] - 32s 103ms/step\n",
            "Epoch: 98 | Discriminator Loss: 0.755236 | Generator Loss: 0.944905 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tJFSFYPXuH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8dmofSbg5ko"
      },
      "source": [
        "o1=s1.get_layer(\"reqs1\").output\n",
        "o2=s2.get_layer(\"reqs2\").output\n",
        "output=tensorflow.keras.layers.concatenate([o1,o2])\n",
        "\n",
        "output=Activation('relu')(output)\n",
        "output2=Dropout(0.5)(output) # For reguralization\n",
        "output3=Dense(10,activation=\"softmax\", name=\"d1\")(output2)\n",
        "\n",
        "mm2=Model([s1.get_layer(\"s1\").input,s2.get_layer(\"s2\").input], output3)\n",
        "my_weights=teacher.get_layer('dense_2').get_weights()\n",
        "mm2.get_layer('d1').set_weights(my_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaSX8xHNg5kx"
      },
      "source": [
        "i=0\n",
        "for l in mm2.layers[:len(mm2.layers)-2]:\n",
        "    l.trainable=False\n",
        "#     print(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLz1Oz6dg5ky"
      },
      "source": [
        "mm2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.0002),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bylsRHBzg5kz",
        "outputId": "c574387e-f8e6-47e1-8aac-13675af7cbf5"
      },
      "source": [
        "# Without finetune\n",
        "batch_size = 256\n",
        "mm2_history=mm2.fit([X_train,X_train], Y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=60,\n",
        "          verbose=1,\n",
        "          validation_data=([X_val,X_val], Y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.6355 - accuracy: 0.8147 - val_loss: 0.5309 - val_accuracy: 0.8364\n",
            "Epoch 2/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 0.8139 - val_loss: 0.5309 - val_accuracy: 0.8362\n",
            "Epoch 3/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6331 - accuracy: 0.8156 - val_loss: 0.5299 - val_accuracy: 0.8365\n",
            "Epoch 4/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6317 - accuracy: 0.8138 - val_loss: 0.5305 - val_accuracy: 0.8362\n",
            "Epoch 5/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6351 - accuracy: 0.8144 - val_loss: 0.5311 - val_accuracy: 0.8365\n",
            "Epoch 6/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6298 - accuracy: 0.8149 - val_loss: 0.5311 - val_accuracy: 0.8367\n",
            "Epoch 7/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6258 - accuracy: 0.8170 - val_loss: 0.5305 - val_accuracy: 0.8366\n",
            "Epoch 8/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6341 - accuracy: 0.8163 - val_loss: 0.5303 - val_accuracy: 0.8369\n",
            "Epoch 9/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6341 - accuracy: 0.8152 - val_loss: 0.5298 - val_accuracy: 0.8365\n",
            "Epoch 10/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6329 - accuracy: 0.8131 - val_loss: 0.5302 - val_accuracy: 0.8370\n",
            "Epoch 11/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6317 - accuracy: 0.8155 - val_loss: 0.5299 - val_accuracy: 0.8366\n",
            "Epoch 12/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 0.8155 - val_loss: 0.5307 - val_accuracy: 0.8363\n",
            "Epoch 13/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6228 - accuracy: 0.8166 - val_loss: 0.5299 - val_accuracy: 0.8365\n",
            "Epoch 14/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6296 - accuracy: 0.8169 - val_loss: 0.5308 - val_accuracy: 0.8363\n",
            "Epoch 15/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6272 - accuracy: 0.8175 - val_loss: 0.5310 - val_accuracy: 0.8369\n",
            "Epoch 16/60\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.6257 - accuracy: 0.8181 - val_loss: 0.5306 - val_accuracy: 0.8359\n",
            "Epoch 17/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6330 - accuracy: 0.8140 - val_loss: 0.5298 - val_accuracy: 0.8366\n",
            "Epoch 18/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6327 - accuracy: 0.8167 - val_loss: 0.5301 - val_accuracy: 0.8368\n",
            "Epoch 19/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6231 - accuracy: 0.8173 - val_loss: 0.5306 - val_accuracy: 0.8365\n",
            "Epoch 20/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6203 - accuracy: 0.8184 - val_loss: 0.5304 - val_accuracy: 0.8366\n",
            "Epoch 21/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6367 - accuracy: 0.8142 - val_loss: 0.5303 - val_accuracy: 0.8368\n",
            "Epoch 22/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6328 - accuracy: 0.8174 - val_loss: 0.5302 - val_accuracy: 0.8366\n",
            "Epoch 23/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6243 - accuracy: 0.8196 - val_loss: 0.5311 - val_accuracy: 0.8365\n",
            "Epoch 24/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6210 - accuracy: 0.8184 - val_loss: 0.5314 - val_accuracy: 0.8363\n",
            "Epoch 25/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6221 - accuracy: 0.8179 - val_loss: 0.5302 - val_accuracy: 0.8361\n",
            "Epoch 26/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6342 - accuracy: 0.8155 - val_loss: 0.5301 - val_accuracy: 0.8369\n",
            "Epoch 27/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6210 - accuracy: 0.8169 - val_loss: 0.5319 - val_accuracy: 0.8369\n",
            "Epoch 28/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6328 - accuracy: 0.8143 - val_loss: 0.5315 - val_accuracy: 0.8366\n",
            "Epoch 29/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6192 - accuracy: 0.8200 - val_loss: 0.5322 - val_accuracy: 0.8359\n",
            "Epoch 30/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6231 - accuracy: 0.8168 - val_loss: 0.5312 - val_accuracy: 0.8367\n",
            "Epoch 31/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6257 - accuracy: 0.8196 - val_loss: 0.5304 - val_accuracy: 0.8362\n",
            "Epoch 32/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6319 - accuracy: 0.8154 - val_loss: 0.5306 - val_accuracy: 0.8365\n",
            "Epoch 33/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6352 - accuracy: 0.8151 - val_loss: 0.5309 - val_accuracy: 0.8363\n",
            "Epoch 34/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6297 - accuracy: 0.8157 - val_loss: 0.5317 - val_accuracy: 0.8363\n",
            "Epoch 35/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6277 - accuracy: 0.8173 - val_loss: 0.5318 - val_accuracy: 0.8361\n",
            "Epoch 36/60\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.6281 - accuracy: 0.8158 - val_loss: 0.5306 - val_accuracy: 0.8367\n",
            "Epoch 37/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6243 - accuracy: 0.8166 - val_loss: 0.5311 - val_accuracy: 0.8365\n",
            "Epoch 38/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6277 - accuracy: 0.8172 - val_loss: 0.5306 - val_accuracy: 0.8366\n",
            "Epoch 39/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6236 - accuracy: 0.8177 - val_loss: 0.5307 - val_accuracy: 0.8367\n",
            "Epoch 40/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 0.8162 - val_loss: 0.5306 - val_accuracy: 0.8370\n",
            "Epoch 41/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6298 - accuracy: 0.8161 - val_loss: 0.5299 - val_accuracy: 0.8372\n",
            "Epoch 42/60\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.6223 - accuracy: 0.8184 - val_loss: 0.5306 - val_accuracy: 0.8367\n",
            "Epoch 43/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6291 - accuracy: 0.8159 - val_loss: 0.5305 - val_accuracy: 0.8369\n",
            "Epoch 44/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 0.8162 - val_loss: 0.5298 - val_accuracy: 0.8364\n",
            "Epoch 45/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6229 - accuracy: 0.8195 - val_loss: 0.5296 - val_accuracy: 0.8365\n",
            "Epoch 46/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6249 - accuracy: 0.8181 - val_loss: 0.5303 - val_accuracy: 0.8362\n",
            "Epoch 47/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6263 - accuracy: 0.8173 - val_loss: 0.5310 - val_accuracy: 0.8360\n",
            "Epoch 48/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6192 - accuracy: 0.8186 - val_loss: 0.5308 - val_accuracy: 0.8361\n",
            "Epoch 49/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6282 - accuracy: 0.8183 - val_loss: 0.5316 - val_accuracy: 0.8364\n",
            "Epoch 50/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6332 - accuracy: 0.8174 - val_loss: 0.5315 - val_accuracy: 0.8361\n",
            "Epoch 51/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6344 - accuracy: 0.8166 - val_loss: 0.5311 - val_accuracy: 0.8355\n",
            "Epoch 52/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6306 - accuracy: 0.8173 - val_loss: 0.5320 - val_accuracy: 0.8362\n",
            "Epoch 53/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6327 - accuracy: 0.8149 - val_loss: 0.5310 - val_accuracy: 0.8366\n",
            "Epoch 54/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6229 - accuracy: 0.8166 - val_loss: 0.5322 - val_accuracy: 0.8358\n",
            "Epoch 55/60\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.6272 - accuracy: 0.8158 - val_loss: 0.5307 - val_accuracy: 0.8365\n",
            "Epoch 56/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6273 - accuracy: 0.8167 - val_loss: 0.5313 - val_accuracy: 0.8360\n",
            "Epoch 57/60\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6336 - accuracy: 0.8155 - val_loss: 0.5310 - val_accuracy: 0.8365\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58/60\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.6307 - accuracy: 0.8152 - val_loss: 0.5312 - val_accuracy: 0.8358\n",
            "Epoch 59/60\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.6298 - accuracy: 0.8164 - val_loss: 0.5306 - val_accuracy: 0.8359\n",
            "Epoch 60/60\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.6230 - accuracy: 0.8183 - val_loss: 0.5312 - val_accuracy: 0.8357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqFAAug_hqBz",
        "outputId": "1037a8ce-9d5e-44fe-f80f-157f8177c8d3"
      },
      "source": [
        "l,a = mm2.evaluate([X_test,X_test], y_test)\n",
        "l, a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.5603 - accuracy: 0.8309\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5603322982788086, 0.8309000134468079)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "Jt5_Vmlwh3r5",
        "outputId": "a250b98a-78c3-482f-c5b9-ebcf7a1be0bf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mm2_history.history['accuracy'])\n",
        "plt.plot(mm2_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(mm2_history.history['loss'])\n",
        "plt.plot(mm2_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPpx-XYoh7hY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}